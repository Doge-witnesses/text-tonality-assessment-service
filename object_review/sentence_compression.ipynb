{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/site-packages (0.1.97)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.1-cp310-cp310-macosx_10_9_x86_64.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m495.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 kB\u001b[0m \u001b[31m414.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/rudovichkiril/Library/Python/3.10/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.5.1 pytz-2022.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install sentencepiece\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "rubert_tiny2_name = 'cointegrated/rubert-tiny2-sentence-compression'\n",
    "rubert_tiny2_model = AutoModelForTokenClassification.from_pretrained(rubert_tiny2_name)\n",
    "rubert_tiny2_tokenizer = AutoTokenizer.from_pretrained(rubert_tiny2_name)\n",
    "\n",
    "def rubert_tiny2_compress(text, threshold=0.5, keep_ratio=None, all=False):\n",
    "    \"\"\" Compress a sentence by removing the least important words.\n",
    "    Parameters:\n",
    "        threshold: cutoff for predicted probabilities of word removal\n",
    "        keep_ratio: proportion of words to preserve\n",
    "    By default, threshold of 0.5 is used.\n",
    "    \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        tok = rubert_tiny2_tokenizer(text, return_tensors='pt').to(rubert_tiny2_model.device)\n",
    "        proba = torch.softmax(rubert_tiny2_model(**tok).logits, -1).cpu().numpy()[0, :, 1]\n",
    "    if keep_ratio is not None:\n",
    "        threshold = sorted(proba)[int(len(proba) * keep_ratio)]\n",
    "    kept_toks = []\n",
    "    keep = False\n",
    "    prev_word_id = None\n",
    "    \n",
    "    if (all):\n",
    "        mp = {}\n",
    "        for word_id, score, token in zip(tok.word_ids(), proba, tok.input_ids[0]):\n",
    "            mp[score] = token\n",
    "        \n",
    "        list_ = list()\n",
    "        for i in sorted(mp):\n",
    "            list_.append(mp[i])\n",
    "        \n",
    "        return rubert_tiny2_tokenizer.decode(list_, skip_special_tokens=True)\n",
    "                \n",
    "    for word_id, score, token in zip(tok.word_ids(), proba, tok.input_ids[0]):\n",
    "        if word_id is None:\n",
    "            keep = True\n",
    "        elif word_id != prev_word_id:\n",
    "            keep = score < threshold\n",
    "        if keep:\n",
    "            kept_toks.append(token)\n",
    "        prev_word_id = word_id\n",
    "    return rubert_tiny2_tokenizer.decode(kept_toks, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "rut5_base_name = 'cointegrated/rut5-base-absum'\n",
    "rut5_base_model = T5ForConditionalGeneration.from_pretrained(rut5_base_name)\n",
    "rut5_base_tokenizer = T5Tokenizer.from_pretrained(rut5_base_name)\n",
    "\n",
    "def rut5_base_compress(\n",
    "    text, n_words=None, compression=None,\n",
    "    max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0, \n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarize the text\n",
    "    The following parameters are mutually exclusive:\n",
    "    - n_words (int) is an approximate number of words to generate.\n",
    "    - compression (float) is an approximate length ratio of summary and original text.\n",
    "    \"\"\"\n",
    "    if n_words:\n",
    "        text = '[{}] '.format(n_words) + text\n",
    "    elif compression:\n",
    "        text = '[{0:.1g}] '.format(compression) + text\n",
    "    x = rut5_base_tokenizer(text, return_tensors='pt', padding=True).to(rut5_base_model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = rut5_base_model.generate(\n",
    "            **x, \n",
    "            max_length=max_length, num_beams=num_beams, \n",
    "            do_sample=do_sample, repetition_penalty=repetition_penalty, \n",
    "            **kwargs\n",
    "        )\n",
    "    return rut5_base_tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "\n",
    "mT5_multilingual_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "mT5_multilingual_tokenizer = AutoTokenizer.from_pretrained(mT5_multilingual_name)\n",
    "mT5_multilingual_model = AutoModelForSeq2SeqLM.from_pretrained(mT5_multilingual_name)\n",
    "\n",
    "def mT5_multilingual_compress(\n",
    "    text\n",
    "):\n",
    "\n",
    "  input_ids = mT5_multilingual_tokenizer(\n",
    "      [WHITESPACE_HANDLER(text)],\n",
    "      return_tensors=\"pt\",\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      max_length=512\n",
    "  )[\"input_ids\"]\n",
    "\n",
    "  output_ids = mT5_multilingual_model.generate(\n",
    "      input_ids=input_ids,\n",
    "      max_length=84,\n",
    "      no_repeat_ngram_size=2,\n",
    "      num_beams=4\n",
    "  )[0]\n",
    "\n",
    "  summary = mT5_multilingual_tokenizer.decode(\n",
    "      output_ids,\n",
    "      skip_special_tokens=True,\n",
    "      clean_up_tokenization_spaces=False\n",
    "  )\n",
    "\n",
    "  return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EncoderDecoderModel\n",
    "\n",
    "rubert_telegram_name = \"IlyaGusev/rubert_telegram_headlines\"\n",
    "rubert_telegram_tokenizer = AutoTokenizer.from_pretrained(rubert_telegram_name, do_lower_case=False, do_basic_tokenize=False, strip_accents=False)\n",
    "rubert_telegram_model = EncoderDecoderModel.from_pretrained(rubert_telegram_name)\n",
    "\n",
    "def rubert_telegram_compress(\n",
    "    text\n",
    "):\n",
    "  input_ids = rubert_telegram_tokenizer(\n",
    "      [text],\n",
    "      add_special_tokens=True,\n",
    "      max_length=256,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_tensors=\"pt\",\n",
    "  )[\"input_ids\"]\n",
    "\n",
    "  output_ids = rubert_telegram_model.generate(\n",
    "      input_ids=input_ids,\n",
    "      max_length=64,\n",
    "      no_repeat_ngram_size=3,\n",
    "      num_beams=10,\n",
    "      top_p=0.95\n",
    "  )[0]\n",
    "\n",
    "  return rubert_telegram_tokenizer.decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref(comment):\n",
    "  return comment.lower().replace(',', '.').replace('!', '.').replace('?', '.').split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_compress(comments): \n",
    "  for comment in comments:\n",
    "    print(comment, '\\n')\n",
    "    rubert_tiny2_compress_comment = str(rubert_tiny2_compress(comment, keep_ratio= min(3/len(comment.split()), 0.9999)))\n",
    "    rut5_base_compress_comment = str(rut5_base_compress(comment, compression=min(3/len(comment.split()), 0.9999)))\n",
    "\n",
    "    print(\"rubert_tiny2_compress: \", rubert_tiny2_compress(comment),\" ## \", rubert_tiny2_compress_comment)\n",
    "    print(\"rut5_base_compress: \", rut5_base_compress(comment),\" ## \", rut5_base_compress_comment)\n",
    "\n",
    "    print('\\n', '='*40, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rubert_tiny2_compress(comments):\n",
    "    for comment in comments:\n",
    "        print(comment)\n",
    "        comment_len = len(comment.split())\n",
    "        for i in range(comment_len):\n",
    "            print(i + 1, \":\", rubert_tiny2_compress(comment, keep_ratio=(i+0.9999)/comment_len))\n",
    "        print('\\n', '='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rut5_base_compress(comments):\n",
    "    for comment in comments:\n",
    "        print(comment)\n",
    "        comment_len = len(comment.split())\n",
    "        for i in range(comment_len):\n",
    "            print(i + 1, \":\", rut5_base_compress(comment, compression=(i+0.9999)/comment_len))\n",
    "        print('\\n', '='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiny_test_DF_rubert_tiny2_compress(comments):\n",
    "    comment_objects_small = [] \n",
    "    comment_objects_medium = []\n",
    "    comment_objects_auto = []\n",
    "    \n",
    "    for comment in comments:\n",
    "        comment_len = len(comment.split())\n",
    "        comment_objects_small.append(rubert_tiny2_compress(comment, keep_ratio=min(2/comment_len,0.999)))\n",
    "        comment_objects_medium.append(rubert_tiny2_compress(comment, keep_ratio=1/2))\n",
    "        comment_objects_auto.append(rubert_tiny2_compress(comment))\n",
    "    \n",
    "    return comment_objects_small, comment_objects_medium, comment_objects_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"wildberries_with_predict.csv\")\n",
    "\n",
    "comment_objects_small, comment_objects_medium, comment_objects_auto = tiny_test_DF_rubert_tiny2_compress(comments['text'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [122], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m comment_objects_DF \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m      2\u001b[0m     data\u001b[39m=\u001b[39m[comments[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list(), comment_objects_small, comment_objects_medium,comment_objects_auto],\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m comment_objects_DF \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(comment_objects_DF\u001b[39m.\u001b[39;49mto_list(), columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msmall\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmedium\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_list'"
     ]
    }
   ],
   "source": [
    "comment_objects_DF = pd.DataFrame(\n",
    "    data=[comments['text'].to_list(), comment_objects_small, comment_objects_medium,comment_objects_auto].T,\n",
    ")\n",
    "\n",
    "comment_objects_DF = pd.DataFrame(comment_objects_DF., columns=['text', 'small', 'medium', 'auto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>small</th>\n",
       "      <th>medium</th>\n",
       "      <th>auto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text  small  medium  auto\n",
       "0   NaN    NaN     NaN   NaN\n",
       "1   NaN    NaN     NaN   NaN\n",
       "2   NaN    NaN     NaN   NaN\n",
       "3   NaN    NaN     NaN   NaN"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_objects_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Всё норм. Мужу понравилось, как всегда.', 'Все пришло попробовал отлично', 'Супер! Оригинал, выгодней намного чем в магазине. Спасибо!', 'Шикарные лезвия, 4 штуки в упаковке (фоткала уже после использования одного). Качество - огонь, острые, прям реально скользят. Лучше, чем покупала в магазинах города. Однозначно буду брать только у этого продавца в дальнейшем.', 'На первый взгляд очень даже хорошие кассеты. Посмотрим, как в  деле..']\n"
     ]
    }
   ],
   "source": [
    "print(comments['text'].head().to_list())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
