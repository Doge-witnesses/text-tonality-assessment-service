{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/site-packages (0.1.97)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.1-cp310-cp310-macosx_10_9_x86_64.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m495.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 kB\u001b[0m \u001b[31m414.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/rudovichkiril/Library/Python/3.10/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.5.1 pytz-2022.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install sentencepiece\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель: rubert-tiny2-sentence-compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "rubert_tiny2_name = 'cointegrated/rubert-tiny2-sentence-compression'\n",
    "rubert_tiny2_model = AutoModelForTokenClassification.from_pretrained(rubert_tiny2_name)\n",
    "rubert_tiny2_tokenizer = AutoTokenizer.from_pretrained(rubert_tiny2_name)\n",
    "\n",
    "def rubert_tiny2_compress(text, threshold=0.5, keep_ratio=None, all=False):\n",
    "    \"\"\" Compress a sentence by removing the least important words.\n",
    "    Parameters:\n",
    "        threshold: cutoff for predicted probabilities of word removal\n",
    "        keep_ratio: proportion of words to preserve\n",
    "    By default, threshold of 0.5 is used.\n",
    "    \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        tok = rubert_tiny2_tokenizer(text, return_tensors='pt').to(rubert_tiny2_model.device)\n",
    "        proba = torch.softmax(rubert_tiny2_model(**tok).logits, -1).cpu().numpy()[0, :, 1]\n",
    "    if keep_ratio is not None:\n",
    "        threshold = sorted(proba)[int(len(proba) * keep_ratio)]\n",
    "    kept_toks = []\n",
    "    keep = False\n",
    "    prev_word_id = None\n",
    "    \n",
    "    if (all):\n",
    "        mp = {}\n",
    "        for word_id, score, token in zip(tok.word_ids(), proba, tok.input_ids[0]):\n",
    "            mp[score] = token\n",
    "        \n",
    "        list_ = list()\n",
    "        for i in sorted(mp):\n",
    "            list_.append(mp[i])\n",
    "        \n",
    "        return rubert_tiny2_tokenizer.decode(list_, skip_special_tokens=True)\n",
    "                \n",
    "    for word_id, score, token in zip(tok.word_ids(), proba, tok.input_ids[0]):\n",
    "        if word_id is None:\n",
    "            keep = True\n",
    "        elif word_id != prev_word_id:\n",
    "            keep = score < threshold\n",
    "        if keep:\n",
    "            kept_toks.append(token)\n",
    "        prev_word_id = word_id\n",
    "    return rubert_tiny2_tokenizer.decode(kept_toks, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель: rut5-base-absum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "rut5_base_name = 'cointegrated/rut5-base-absum'\n",
    "rut5_base_model = T5ForConditionalGeneration.from_pretrained(rut5_base_name)\n",
    "rut5_base_tokenizer = T5Tokenizer.from_pretrained(rut5_base_name)\n",
    "\n",
    "def rut5_base_compress(\n",
    "    text, n_words=None, compression=None,\n",
    "    max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0, \n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarize the text\n",
    "    The following parameters are mutually exclusive:\n",
    "    - n_words (int) is an approximate number of words to generate.\n",
    "    - compression (float) is an approximate length ratio of summary and original text.\n",
    "    \"\"\"\n",
    "    if n_words:\n",
    "        text = '[{}] '.format(n_words) + text\n",
    "    elif compression:\n",
    "        text = '[{0:.1g}] '.format(compression) + text\n",
    "    x = rut5_base_tokenizer(text, return_tensors='pt', padding=True).to(rut5_base_model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = rut5_base_model.generate(\n",
    "            **x, \n",
    "            max_length=max_length, num_beams=num_beams, \n",
    "            do_sample=do_sample, repetition_penalty=repetition_penalty, \n",
    "            **kwargs\n",
    "        )\n",
    "    return rut5_base_tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель: mT5_multilingual_XLSum (-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "\n",
    "# mT5_multilingual_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "# mT5_multilingual_tokenizer = AutoTokenizer.from_pretrained(mT5_multilingual_name)\n",
    "# mT5_multilingual_model = AutoModelForSeq2SeqLM.from_pretrained(mT5_multilingual_name)\n",
    "\n",
    "# def mT5_multilingual_compress(\n",
    "#     text\n",
    "# ):\n",
    "\n",
    "#   input_ids = mT5_multilingual_tokenizer(\n",
    "#       [WHITESPACE_HANDLER(text)],\n",
    "#       return_tensors=\"pt\",\n",
    "#       padding=\"max_length\",\n",
    "#       truncation=True,\n",
    "#       max_length=512\n",
    "#   )[\"input_ids\"]\n",
    "\n",
    "#   output_ids = mT5_multilingual_model.generate(\n",
    "#       input_ids=input_ids,\n",
    "#       max_length=84,\n",
    "#       no_repeat_ngram_size=2,\n",
    "#       num_beams=4\n",
    "#   )[0]\n",
    "\n",
    "#   summary = mT5_multilingual_tokenizer.decode(\n",
    "#       output_ids,\n",
    "#       skip_special_tokens=True,\n",
    "#       clean_up_tokenization_spaces=False\n",
    "#   )\n",
    "\n",
    "#   return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель: rubert_telegram_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, EncoderDecoderModel\n",
    "\n",
    "# rubert_telegram_name = \"IlyaGusev/rubert_telegram_headlines\"\n",
    "# rubert_telegram_tokenizer = AutoTokenizer.from_pretrained(rubert_telegram_name, do_lower_case=False, do_basic_tokenize=False, strip_accents=False)\n",
    "# rubert_telegram_model = EncoderDecoderModel.from_pretrained(rubert_telegram_name)\n",
    "\n",
    "# def rubert_telegram_compress(\n",
    "#     text\n",
    "# ):\n",
    "#   input_ids = rubert_telegram_tokenizer(\n",
    "#       [text],\n",
    "#       add_special_tokens=True,\n",
    "#       max_length=256,\n",
    "#       padding=\"max_length\",\n",
    "#       truncation=True,\n",
    "#       return_tensors=\"pt\",\n",
    "#   )[\"input_ids\"]\n",
    "\n",
    "#   output_ids = rubert_telegram_model.generate(\n",
    "#       input_ids=input_ids,\n",
    "#       max_length=64,\n",
    "#       no_repeat_ngram_size=3,\n",
    "#       num_beams=10,\n",
    "#       top_p=0.95\n",
    "#   )[0]\n",
    "\n",
    "#   return rubert_telegram_tokenizer.decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref(comment):\n",
    "  return comment.lower().replace(',', '.').replace('!', '.').replace('?', '.').split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_compress(comments): \n",
    "  for comment in comments:\n",
    "    print(comment, '\\n')\n",
    "    rubert_tiny2_compress_comment = str(rubert_tiny2_compress(comment, keep_ratio= min(3/len(comment.split()), 0.9999)))\n",
    "    rut5_base_compress_comment = str(rut5_base_compress(comment, compression=min(3/len(comment.split()), 0.9999)))\n",
    "\n",
    "    print(\"rubert_tiny2_compress: \", rubert_tiny2_compress(comment),\" ## \", rubert_tiny2_compress_comment)\n",
    "    print(\"rut5_base_compress: \", rut5_base_compress(comment),\" ## \", rut5_base_compress_comment)\n",
    "\n",
    "    print('\\n', '='*40, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rubert_tiny2_compress(comments):\n",
    "    for comment in comments:\n",
    "        print(comment)\n",
    "        comment_len = len(comment.split())\n",
    "        for i in range(comment_len):\n",
    "            print(i + 1, \":\", rubert_tiny2_compress(comment, keep_ratio=(i+0.9999)/comment_len))\n",
    "        print('\\n', '='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rut5_base_compress(comments):\n",
    "    for comment in comments:\n",
    "        print(comment)\n",
    "        comment_len = len(comment.split())\n",
    "        for i in range(comment_len):\n",
    "            print(i + 1, \":\", rut5_base_compress(comment, compression=(i+0.9999)/comment_len))\n",
    "        print('\\n', '='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiny_test_DF_rubert_tiny2_compress(comments):\n",
    "    comment_objects_small = [] \n",
    "    comment_objects_medium = []\n",
    "    comment_objects_auto = []\n",
    "    \n",
    "    for comment in comments:\n",
    "        comment_len = len(comment.split())\n",
    "        comment_objects_small.append(rubert_tiny2_compress(comment, keep_ratio=min(2/comment_len,0.999)))\n",
    "        comment_objects_medium.append(rubert_tiny2_compress(comment, keep_ratio=1/2))\n",
    "        comment_objects_auto.append(rubert_tiny2_compress(comment))\n",
    "    \n",
    "    return comment_objects_small, comment_objects_medium, comment_objects_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"wildberries_with_predict.csv\")\n",
    "\n",
    "comment_objects_small, comment_objects_medium, comment_objects_auto = tiny_test_DF_rubert_tiny2_compress(comments['text'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_objects_DF = pd.DataFrame(\n",
    "    data=np.array([comments['text'].to_list(), comment_objects_small, comment_objects_medium,comment_objects_auto]).T,\n",
    "    columns=['text', 'small', 'medium', 'auto']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>small</th>\n",
       "      <th>medium</th>\n",
       "      <th>auto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Всё норм. Мужу понравилось, как всегда.</td>\n",
       "      <td>..</td>\n",
       "      <td>Всё норм..</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Все пришло попробовал отлично</td>\n",
       "      <td>попробовал отлично</td>\n",
       "      <td>попробовал отлично</td>\n",
       "      <td>Все пришло попробовал отлично</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Супер! Оригинал, выгодней намного чем в магази...</td>\n",
       "      <td>Супер!!</td>\n",
       "      <td>Супер! Оригинал.!</td>\n",
       "      <td>Супер! Оригинал!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Шикарные лезвия, 4 штуки в упаковке (фоткала у...</td>\n",
       "      <td>лезвия.</td>\n",
       "      <td>Шикарные лезвия, 4 штуки в упаковке. Качество ...</td>\n",
       "      <td>Шикарные лезвия, 4 штуки в упаковке. Качество ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>На первый взгляд очень даже хорошие кассеты. П...</td>\n",
       "      <td>На..</td>\n",
       "      <td>На первый кассеты...</td>\n",
       "      <td>На..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70078</th>\n",
       "      <td>Для тренировок в сммый раз</td>\n",
       "      <td>Для тренировок раз</td>\n",
       "      <td>Для тренировок раз</td>\n",
       "      <td>Для тренировок в сммый раз</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70079</th>\n",
       "      <td>Хорошая майка</td>\n",
       "      <td>Хорошая майка</td>\n",
       "      <td>майка</td>\n",
       "      <td>Хорошая майка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70080</th>\n",
       "      <td>Майка не понравилась-очень тонкий материал!</td>\n",
       "      <td>Майка!</td>\n",
       "      <td>Майка не!</td>\n",
       "      <td>Майка не понравилась - тонкий материал!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70081</th>\n",
       "      <td>На рост 190 см Вес 94 кг .подошел отлично разм...</td>\n",
       "      <td>На</td>\n",
       "      <td>На рост 190 см Вес 94 кг. ХL</td>\n",
       "      <td>На рост 190 см Вес кг</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70082</th>\n",
       "      <td>Материал очень приятный , качество хорошее , м...</td>\n",
       "      <td>Материал понравилась</td>\n",
       "      <td>Материал приятный понравилась</td>\n",
       "      <td>Материал приятный мужу понравилась</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70083 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0                Всё норм. Мужу понравилось, как всегда.   \n",
       "1                          Все пришло попробовал отлично   \n",
       "2      Супер! Оригинал, выгодней намного чем в магази...   \n",
       "3      Шикарные лезвия, 4 штуки в упаковке (фоткала у...   \n",
       "4      На первый взгляд очень даже хорошие кассеты. П...   \n",
       "...                                                  ...   \n",
       "70078                         Для тренировок в сммый раз   \n",
       "70079                                      Хорошая майка   \n",
       "70080        Майка не понравилась-очень тонкий материал!   \n",
       "70081  На рост 190 см Вес 94 кг .подошел отлично разм...   \n",
       "70082  Материал очень приятный , качество хорошее , м...   \n",
       "\n",
       "                      small  \\\n",
       "0                        ..   \n",
       "1        попробовал отлично   \n",
       "2                   Супер!!   \n",
       "3                   лезвия.   \n",
       "4                      На..   \n",
       "...                     ...   \n",
       "70078    Для тренировок раз   \n",
       "70079         Хорошая майка   \n",
       "70080                Майка!   \n",
       "70081                    На   \n",
       "70082  Материал понравилась   \n",
       "\n",
       "                                                  medium  \\\n",
       "0                                             Всё норм..   \n",
       "1                                     попробовал отлично   \n",
       "2                                      Супер! Оригинал.!   \n",
       "3      Шикарные лезвия, 4 штуки в упаковке. Качество ...   \n",
       "4                                   На первый кассеты...   \n",
       "...                                                  ...   \n",
       "70078                                 Для тренировок раз   \n",
       "70079                                              майка   \n",
       "70080                                          Майка не!   \n",
       "70081                       На рост 190 см Вес 94 кг. ХL   \n",
       "70082                      Материал приятный понравилась   \n",
       "\n",
       "                                                    auto  \n",
       "0                                                      .  \n",
       "1                          Все пришло попробовал отлично  \n",
       "2                                       Супер! Оригинал!  \n",
       "3      Шикарные лезвия, 4 штуки в упаковке. Качество ...  \n",
       "4                                                   На..  \n",
       "...                                                  ...  \n",
       "70078                         Для тренировок в сммый раз  \n",
       "70079                                      Хорошая майка  \n",
       "70080            Майка не понравилась - тонкий материал!  \n",
       "70081                              На рост 190 см Вес кг  \n",
       "70082                 Материал приятный мужу понравилась  \n",
       "\n",
       "[70083 rows x 4 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_objects_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_objects_DF.to_csv('wildberries_comment_objects_example_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
