# Дообученная модель DeepPavlov/rubert-base-cased
## Описание модели
RuBERT (русский, 12-слойный, 768- размер скрытого слоя скрытый, 12-головной, 180М параметров) обучался на русскоязычной части Википедии и новостных данных. На этих обучающих данных мы построили словарь русских субтокенов и взяли мультиязычную версию BERT‑базы в качестве инициализации для RuBERT.
