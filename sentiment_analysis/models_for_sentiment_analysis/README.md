# Модели для оценки тональности
Для оценки разметки данных и оценки тональности использовались 4 модели. Первые 3 модели были обучены и протестированы на корпусе women_with_predict.
### Описание корпуса women_with_predict:

1. Объем корпуса: 90001
2. разметка с оценкой (2 - отрицательный, 0 - нейтральный, 1 - положительный)
3. Примерная длина документов (словами): 25

# Модели:
## №1 DeepPavlov/rubert-base-cased

### Описание модели
RuBERT (русский, 12-слойный, 768-размер скрытого слоя скрытый, 12-головной, 180М параметров) обучался на русскоязычной части Википедии и новостных данных. На этих обучающих данных мы построили словарь русских субтокенов и взяли мультиязычную версию BERT‑базы в качестве инициализации для RuBERT.
## №2 DeepPavlov/rubert-base-cased-sentence
К данной модели был добавлен полносвязный слой.
### Описание модели
Sentence RuBERT (русский, 12-слойный, 768-размер скрытого слоя, 12-заголовочный, 180M параметров) — кодировщик предложений для русского языка на основе представления. Он инициализирован с помощью RuBERT и настроен на SNLI с гугл-переводом на русский и на русскоязычной части XNLI.
## №3 xlm-roberta-base

### Описание модели
Sentence RuBERT (русский, 12-слойный, 768-размер скрытого слоя, 12-заголовочный, 180M параметров) — кодировщик предложений для русского языка на основе представления. Он инициализирован с помощью RuBERT и настроен на SNLI с гугл-переводом на русский и на русскоязычной части XNLI.

## №4 Twitter/twhin-bert-base
Эта модель была обучена на предварительно сбалансированных корпусах: hotel, women, reviews, phone
### Описание корпусов
Отзывы отеля:
1. Объем корпуса: 6877 + 50329
2. отзывы на отель
3. бинарная 
4. Примерная длина документов (словами): 35

Телефоны
1. Объем корпуса: 458433
2. телефоны
3. балльная оценка
4. Примерная длина документов (словами): 100

Отзывы из магазина женской одежды:
1. Объем корпуса: 90001
2. отзывы из магазина женской одежды
3. разметка с оценкой
4. Примерная длина документов (словами): 20

### Описание модели
TwHIN-BERT — это новая многоязычная языковая модель твитов, которая обучается на 7 миллиардах твитов на более чем 100 различных языках. TwHIN-BERT отличается от предыдущих предварительно обученных языковых моделей тем, что обучается не только с помощью текстового самоконтроля (например, MLM), но и с социальной целью, основанной на богатом социальном взаимодействии в гетерогенной информационной сети Twitter.
280M параметров
