# Модели для оценки тональности
## Для оценки разметки данных и оценки тональности использовались 4 модели. Первые 3 модели были обучены и протестированы на корпусе women_with_predict.
### Описание корпуса women_with_predict:

1. Объем корпуса: 90001
2. разметка с оценкой (2 - отрицательный, 0 - нейтральный, 1 - положительный)
3. Примерная длина документов (словами): 25

# Модели:
## №1 DeepPavlov/rubert-base-cased

### Описание модели
RuBERT (русский, 12-слойный, 768-размер скрытого слоя скрытый, 12-головной, 180М параметров) обучался на русскоязычной части Википедии и новостных данных. На этих обучающих данных мы построили словарь русских субтокенов и взяли мультиязычную версию BERT‑базы в качестве инициализации для RuBERT.

### Результаты:

              precision    recall  f1-score   support

     Neutral       0.64      0.71      0.67      5000
    Positive       0.92      0.86      0.89      5427
    Negative       0.76      0.74      0.75      5218
    accuracy                           0.77     15645

## №2 DeepPavlov/rubert-base-cased-sentence
К данной модели был добавлен полносвязный слой.
### Описание модели
Sentence RuBERT (русский, 12-слойный, 768-размер скрытого слоя, 12-заголовочный, 180M параметров) — кодировщик предложений для русского языка на основе представления. Он инициализирован с помощью RuBERT и настроен на SNLI с гугл-переводом на русский и на русскоязычной части XNLI.

### Результаты:

              precision    recall  f1-score   support

     Neutral       0.66      0.69      0.67      5338
    Positive       0.92      0.86      0.89      5375
    Negative       0.75      0.75      0.75      5387
    accuracy                           0.77     16100

## №3 xlm-roberta-base

### Описание модели
Sentence RuBERT (русский, 12-слойный, 768-размер скрытого слоя, 12-заголовочный, 180M параметров) — кодировщик предложений для русского языка на основе представления. Он инициализирован с помощью RuBERT и настроен на SNLI с гугл-переводом на русский и на русскоязычной части XNLI.

### Результаты:

              precision    recall  f1-score   support

     Neutral       0.64      0.70      0.67      2730
    Positive       0.89      0.84      0.86      2729
    Negative       0.74      0.71      0.72      2591
    accuracy                           0.75      8050

## №4 Twitter/twhin-bert-base
Эта модель была обучена на предварительно сбалансированных корпусах: hotel, women, reviews, phone
### Описание корпусов

### Отзывы отеля:

    Объем корпуса: 6877 + 50329
    Предметная область: отзывы на отель
    Разделение лэйблов: бинарная
    Примерная длина документов (словами): 35

### Отзывы на телефоны:

    Объем корпуса: 458433
    Предметная область: телефоны
    Разделение лэйблов: балльная оценка
    Примерная длина документов (словами): 100

### Отзывы из магазина женской одежды:

    Объем корпуса: 90001
    Предметная область: отзывы из магазина женской одежды
    Разделение лэйблов: разметка с оценкой
    Примерная длина документов (словами): 20

### !!! reviews:

    Объем корпуса: ???
    Предметная область: ???
    Разделение лэйблов: ???
    Примерная длина документов (словами): 70


### Описание модели
TwHIN-BERT — это новая многоязычная языковая модель твитов, которая обучается на 7 миллиардах твитов на более чем 100 различных языках. TwHIN-BERT отличается от предыдущих предварительно обученных языковых моделей тем, что обучается не только с помощью текстового самоконтроля (например, MLM), но и с социальной целью, основанной на богатом социальном взаимодействии в гетерогенной информационной сети Twitter.
280M параметров

## Проверка качества модели:

### На всех данных:

              precision    recall  f1-score   support

     Neutral       0.75      0.72      0.74      7189
    Positive       0.82      0.88      0.85      6664
    Negative       0.87      0.84      0.85      7078
    accuracy                           0.81     20931

    
### На корпусе hotel:

              precision    recall  f1-score   support

           0       0.25      0.24      0.25      1954
           1       0.74      0.37      0.49      1894
           2       0.52      0.79      0.63      2005
    accuracy                           0.47      5853

### На корпусе phone:

              precision    recall  f1-score   support

           0       0.25      0.24      0.25      1954
           1       0.74      0.37      0.49      1894
           2       0.52      0.79      0.63      2005
    accuracy                           0.47      5853

### На корпусе women:

              precision    recall  f1-score   support

           0       0.60      0.68      0.64      5000
           1       0.92      0.83      0.87      5427
           2       0.74      0.72      0.73      5218
    accuracy                           0.75     15645

### На корпусе reviews:

              precision    recall  f1-score   support

           0       0.75      0.80      0.78      1414
           1       0.93      0.82      0.87      1356
           2       0.86      0.89      0.87      1416
    accuracy                           0.84      4186
