{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IwM3A8gTu48d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7c7dca-6b1e-4d2e-8fab-ce803ac5f7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 36.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 68.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 81.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "!pip install transformers\n",
        "import random\n",
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DF = pd.read_csv('/content/women_balanced.csv',encoding='utf-8',lineterminator='\\n')\n",
        "\n",
        "\n",
        "#DF['label'] = DF['label'].apply(lambda x: x+1)\n",
        "data = DF[['text','label']]\n",
        "\n",
        "\n",
        "train_text, test_text, train_labels, test_labels = train_test_split(data[['text']], data[['label']], test_size=0.2, random_state=42)\n",
        "train_text, val_text, train_labels, val_labels = train_test_split(train_text, train_labels, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "Ide9NtnkD58a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert = AutoModel.from_pretrained('DeepPavlov/rubert-base-cased-sentence')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased-sentence')"
      ],
      "metadata": {
        "id": "pEYJ0Zn2EfHU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = [len(str(i).split()) for i in train_text['text']]\n",
        "pd.Series(seq_len).hist(bins = 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ULjqo9MeoQfS",
        "outputId": "08790e75-3ae3-483b-f6ff-d3b70dc6d3e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f40e4dfb8b0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = train_text['text'].astype('str')\n",
        "train_labels = train_labels['label'].astype('int')\n",
        "\n",
        "val_text = val_text['text'].astype('str')\n",
        "val_labels = val_labels['label'].astype('int')\n",
        "\n",
        "test_text = test_text['text'].astype('str')\n",
        "test_labels = test_labels['label'].astype('int')"
      ],
      "metadata": {
        "id": "NeBg4R0jo5c_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    list(train_text.values),\n",
        "    max_length = 65,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    list(val_text.values),\n",
        "    max_length = 65,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    list(test_text.values),\n",
        "    max_length = 65,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.values)\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.values)\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.values)\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
        "\n",
        "val_data =  TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_seq, test_mask)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "_oNqgLTj2tm7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "    \n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        self.fc2 = nn.Linear(512,3)\n",
        "        self.softmax = nn.Softmax(dim = -1)\n",
        "    \n",
        "    def forward(self, sent_id, mask):\n",
        "        _, cls_hs = self.bert(sent_id, attention_mask = mask, return_dict = False)\n",
        "        x = self.fc1(cls_hs)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oMBVJOYg-8Gu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT_Arch(bert)\n",
        "\n",
        "model = model.to(device)\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C_DXcYdGb_y",
        "outputId": "a7ce3f8b-ae17-48b2-f373-d161cead0041"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "print(class_weights)\n",
        "\n",
        "weights = torch.tensor(class_weights, dtype = torch.float)\n",
        "weights = weights.to(device)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "epochs = 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUNqwhM3GicJ",
        "outputId": "5e5a8fa9-de2f-4e79-ae0c-c6cace4aa77a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.00844399 0.99194776 0.99974439]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = []\n",
        "  \n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total = len(train_dataloader)):\n",
        "        \n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id,mask,labels = batch\n",
        "        model.zero_grad()\n",
        "        preds = model(sent_id, mask)\n",
        "        loss = cross_entropy(preds, labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        total_preds.append(preds)\n",
        "        \n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    total_preds = np.concatenate(total_preds, axis = 0)\n",
        "    \n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "rMElXb6qUjPB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss, total_accuracy = 0,0\n",
        "    total_preds = []\n",
        "\n",
        "    for step, batch in tqdm(enumerate(val_dataloader), total = len(val_dataloader)):\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            preds = model(sent_id, mask)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "            preds = preds.detach().cpu().argmax(axis=1).numpy()\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    total_preds = np.concatenate(total_preds, axis = 0)\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "529hgcu_N3gl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('\\n Epoch{:} / {:}'.format(epoch+1, epochs))\n",
        "    \n",
        "    train_loss, _ = train()\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    print(f'\\nTraining loss: {train_loss:.3f}')\n",
        "    print(f'Validation loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0Fcx_YgN592",
        "outputId": "07d72f8c-6109-48ac-ca8e-d9ab66956287"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch1 / 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1467/1467 [03:13<00:00,  7.57it/s]\n",
            "100%|██████████| 489/489 [01:03<00:00,  7.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training loss: 0.936\n",
            "Validation loss: 0.886\n",
            "\n",
            " Epoch2 / 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1467/1467 [03:12<00:00,  7.61it/s]\n",
            "100%|██████████| 489/489 [01:03<00:00,  7.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training loss: 0.883\n",
            "Validation loss: 0.874\n",
            "\n",
            " Epoch3 / 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1467/1467 [03:12<00:00,  7.63it/s]\n",
            "100%|██████████| 489/489 [01:02<00:00,  7.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training loss: 0.873\n",
            "Validation loss: 0.867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "SGw0tAAbWlps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b90cb8c-ed71-4383-b010-97e98f3691f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from datetime import timedelta\n",
        "import time\n",
        "import sklearn.metrics\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "list_seq = np.array_split(test_seq, 50)\n",
        "list_mask = np.array_split(test_mask, 50)\n",
        "start = time.time()\n",
        "\n",
        "predictions = []\n",
        "for num, elem in enumerate(list_seq):\n",
        "    with torch.no_grad():\n",
        "        preds = model(elem.to(device), list_mask[num].to(device))\n",
        "        predictions.append(preds.detach().cpu().argmax(axis=1).numpy())"
      ],
      "metadata": {
        "id": "Wj2ByklWWpa8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_preds = []\n",
        "for i in predictions:\n",
        "  for j in i:\n",
        "    flat_preds.append(j)\n",
        "\n",
        "data_label = test_labels.to_list()\n",
        "print(classification_report(test_labels ,flat_preds))"
      ],
      "metadata": {
        "id": "dqjLR3bfSC2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1f92f7-e458-45ac-a487-353d867bd925"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.54      0.55      5182\n",
            "           1       0.77      0.79      0.78      5237\n",
            "           2       0.67      0.69      0.68      5227\n",
            "\n",
            "    accuracy                           0.67     15646\n",
            "   macro avg       0.67      0.67      0.67     15646\n",
            "weighted avg       0.67      0.67      0.67     15646\n",
            "\n"
          ]
        }
      ]
    }
  ]
}