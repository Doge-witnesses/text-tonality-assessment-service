{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Загрузка и подготовка данных**"
      ],
      "metadata": {
        "id": "rr0yYhMALi6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FHtFKIYVQVTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799fb2ed-9753-4764-c313-7a195d69296a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "lKt6veuDPVwx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "DF = pd.read_csv('/content/main_data_upd.csv',encoding='utf-8',lineterminator='\\n')\n",
        "\n",
        "data = DF[['text','label']]\n",
        "data = data.dropna()\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "tmp = round(len(data)* 0.6)\n",
        "tmp_1 = round(len(data) * 0.8)\n",
        "\n",
        "train_data = data.iloc[0:tmp]\n",
        "val_data = data.iloc[tmp:tmp_1]\n",
        "test_data = data.iloc[tmp_1:len(data)]\n",
        "\n",
        "train_text = train_data['text'].astype('str')\n",
        "train_labels = train_data['label'].astype('int')\n",
        "val_text = val_data['text'].astype('str')\n",
        "val_labels = val_data['label'].astype('int')\n",
        "test_text = test_data['text'].astype('str')\n",
        "test_labels = test_data['label'].astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Подключение библиотек**"
      ],
      "metadata": {
        "id": "1KYY_7nOMGKf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhghEw_aQzY9",
        "outputId": "55fb8f3a-6676-4fe0-d15a-db16c96f3618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import BertTokenizer, AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загрузка токенизатора с модели DeepPavlov/rubert-base-cased**"
      ],
      "metadata": {
        "id": "8MbXmHjzL0N7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "X5_NLK9rRCxS"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('Twitter/twhin-bert-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**График длин отзывов**"
      ],
      "metadata": {
        "id": "twcg2AmWMXMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = [len(str(i).split()) for i in train_text]\n",
        "pd.Series(seq_len).hist(bins = 50)"
      ],
      "metadata": {
        "id": "yhyTJmvRaaUx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0b3aaafd-bf15-477b-ea3c-a829f6cd5c0c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f794d4c3410>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Токенизация данных**"
      ],
      "metadata": {
        "id": "vcQNFmzFMzlU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Sjck_TJ1RN0j"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    list(train_text.values),\n",
        "    max_length = 120,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    list(val_text.values),\n",
        "    max_length = 120,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    list(test_text.values),\n",
        "    max_length = 120,\n",
        "    padding = 'max_length',\n",
        "    truncation = True\n",
        ")\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.values)\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.values)\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.values)\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
        "\n",
        "val_data =  TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_seq, test_mask)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загружаем модель DeepPavlov/rubert-base-cased, добавляем 3 выхода**"
      ],
      "metadata": {
        "id": "Q9rzUHJxtuiO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X-ErgV9Ry1u",
        "outputId": "14291fea-515d-4453-fb6c-9555fd65345f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Twitter/twhin-bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Twitter/twhin-bert-base and are newly initialized: ['bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"Twitter/twhin-bert-base\", \n",
        "                                                      num_labels = 3, \n",
        "                                                      output_attentions = False,\n",
        "                                                      output_hidden_states = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc9k0R_WSQrA",
        "outputId": "d8000114-eb4f-4c40-cf05-6650c4897e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "UjxDtmhYSTJe"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "n_epochs = 3\n",
        "\n",
        "n_steps = len(train_dataloader) * n_epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = n_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Дообучение модели на датасете**"
      ],
      "metadata": {
        "id": "7nSntJtFNpFZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_P1CmtLSfWX",
        "outputId": "176066df-79d9-4e44-c23c-de4ac7bae875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:\n",
            "Batch   100  of  1,570.    Time: 0:01:13.\n",
            "Batch   200  of  1,570.    Time: 0:02:27.\n",
            "Batch   300  of  1,570.    Time: 0:03:44.\n",
            "Batch   400  of  1,570.    Time: 0:05:01.\n",
            "Batch   500  of  1,570.    Time: 0:06:18.\n",
            "Batch   600  of  1,570.    Time: 0:07:36.\n",
            "Batch   700  of  1,570.    Time: 0:08:54.\n",
            "Batch   800  of  1,570.    Time: 0:10:12.\n",
            "Batch   900  of  1,570.    Time: 0:11:30.\n",
            "Batch 1,000  of  1,570.    Time: 0:12:48.\n",
            "Batch 1,100  of  1,570.    Time: 0:14:05.\n",
            "Batch 1,200  of  1,570.    Time: 0:15:23.\n",
            "Batch 1,300  of  1,570.    Time: 0:16:41.\n",
            "Batch 1,400  of  1,570.    Time: 0:17:58.\n",
            "Batch 1,500  of  1,570.    Time: 0:19:16.\n",
            "Mean loss:  0.0005547886593372812\n",
            "Training epoch took: 0:20:11\n",
            "\n",
            "Validation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.43      0.50      5599\n",
            "           1       0.75      0.75      0.75      5553\n",
            "           2       0.66      0.84      0.74      5591\n",
            "\n",
            "    accuracy                           0.67     16743\n",
            "   macro avg       0.67      0.67      0.66     16743\n",
            "weighted avg       0.67      0.67      0.66     16743\n",
            "\n",
            "Validation took: 0:01:54\n",
            "\n",
            "Training:\n",
            "Batch   100  of  1,570.    Time: 0:01:16.\n",
            "Batch   200  of  1,570.    Time: 0:02:34.\n",
            "Batch   300  of  1,570.    Time: 0:03:52.\n",
            "Batch   400  of  1,570.    Time: 0:05:10.\n",
            "Batch   500  of  1,570.    Time: 0:06:27.\n",
            "Batch   600  of  1,570.    Time: 0:07:45.\n",
            "Batch   700  of  1,570.    Time: 0:09:03.\n",
            "Batch   800  of  1,570.    Time: 0:10:20.\n",
            "Batch   900  of  1,570.    Time: 0:11:38.\n",
            "Batch 1,000  of  1,570.    Time: 0:12:56.\n",
            "Batch 1,100  of  1,570.    Time: 0:14:13.\n",
            "Batch 1,200  of  1,570.    Time: 0:15:31.\n",
            "Batch 1,300  of  1,570.    Time: 0:16:49.\n",
            "Batch 1,400  of  1,570.    Time: 0:18:06.\n",
            "Batch 1,500  of  1,570.    Time: 0:19:24.\n",
            "Mean loss:  0.0004188101518388595\n",
            "Training epoch took: 0:20:19\n",
            "\n",
            "Validation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.62      0.60      5599\n",
            "           1       0.75      0.77      0.76      5553\n",
            "           2       0.78      0.72      0.75      5591\n",
            "\n",
            "    accuracy                           0.70     16743\n",
            "   macro avg       0.71      0.70      0.70     16743\n",
            "weighted avg       0.71      0.70      0.70     16743\n",
            "\n",
            "Validation took: 0:01:54\n",
            "\n",
            "Training:\n",
            "Batch   100  of  1,570.    Time: 0:01:16.\n",
            "Batch   200  of  1,570.    Time: 0:02:34.\n",
            "Batch   300  of  1,570.    Time: 0:03:52.\n",
            "Batch   400  of  1,570.    Time: 0:05:09.\n",
            "Batch   500  of  1,570.    Time: 0:06:27.\n",
            "Batch   600  of  1,570.    Time: 0:07:45.\n",
            "Batch   700  of  1,570.    Time: 0:09:03.\n",
            "Batch   800  of  1,570.    Time: 0:10:20.\n",
            "Batch   900  of  1,570.    Time: 0:11:38.\n",
            "Batch 1,000  of  1,570.    Time: 0:12:55.\n",
            "Batch 1,100  of  1,570.    Time: 0:14:13.\n",
            "Batch 1,200  of  1,570.    Time: 0:15:31.\n",
            "Batch 1,300  of  1,570.    Time: 0:16:48.\n",
            "Batch 1,400  of  1,570.    Time: 0:18:06.\n",
            "Batch 1,500  of  1,570.    Time: 0:19:24.\n",
            "Mean loss:  0.0004627760868705699\n",
            "Training epoch took: 0:20:18\n",
            "\n",
            "Validation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.61      0.60      5599\n",
            "           1       0.76      0.77      0.77      5553\n",
            "           2       0.78      0.74      0.76      5591\n",
            "\n",
            "    accuracy                           0.71     16743\n",
            "   macro avg       0.71      0.71      0.71     16743\n",
            "weighted avg       0.71      0.71      0.71     16743\n",
            "\n",
            "Validation took: 0:01:54\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datetime import timedelta\n",
        "import time\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "seed = 42\n",
        "random.seed = (seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "model.cuda()\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(\"Training:\")\n",
        "    start = time.time()\n",
        "    mean_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        if (step + 1) % 100 == 0:\n",
        "            duration = timedelta(seconds=int(time.time() - start))\n",
        "            print('Batch {:>5,}  of  {:>5,}.    Time: {:}.'.format(step + 1, len(train_dataloader), duration))\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_attention_masks = batch[1].to(device)\n",
        "        b_targets = batch[2].to(device)\n",
        "        model.zero_grad()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        outputs = model(b_input_ids, attention_mask=b_attention_masks, labels = b_targets)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        mean_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        mean_loss = mean_loss / len(train_dataloader)\n",
        "    \n",
        "    losses.append(mean_loss)\n",
        "    print(\"Mean loss: \" , mean_loss)\n",
        "    print(\"Training epoch took:\" , timedelta(seconds=int(time.time() - start)))\n",
        "    \n",
        "    print()\n",
        "    print(\"Validation:\")\n",
        "    model.eval()\n",
        "    \n",
        "    start = time.time()\n",
        "    predictions = torch.Tensor().to(dtype=torch.int8)\n",
        "    \n",
        "    for batch in val_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_attention_masks = batch[1].to(device)\n",
        "        b_targets = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_attention_masks, output_hidden_states=False, output_attentions=False, return_dict=True)\n",
        "        \n",
        "        predictions = torch.cat((predictions, outputs.logits.cpu().argmax(axis=1)))\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    print(classification_report(val_y, predictions))\n",
        "    print(\"Validation took: {:}\".format(timedelta(seconds = int(time.time() - start))))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Тестирование модели**"
      ],
      "metadata": {
        "id": "oLLwLdw7N2qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing:\")\n",
        "model.eval()\n",
        "\n",
        "t0 = time.time()\n",
        "predictions = torch.Tensor().to(dtype=torch.int8)\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_attention_masks = batch[1].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, attention_mask=b_attention_masks, output_hidden_states=False, output_attentions=False, return_dict=True)\n",
        "\n",
        "    predictions = torch.cat((predictions, outputs.logits.cpu().argmax(axis=1)))\n",
        "\n",
        "print(classification_report(test_y ,predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJu96aQoPpRX",
        "outputId": "b38da595-77c8-4868-d937-a035a9d249cd"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.61      0.60      5587\n",
            "           1       0.75      0.76      0.76      5567\n",
            "           2       0.79      0.76      0.77      5589\n",
            "\n",
            "    accuracy                           0.71     16743\n",
            "   macro avg       0.71      0.71      0.71     16743\n",
            "weighted avg       0.71      0.71      0.71     16743\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UJ9sg8q3iJ9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/model_upd.pth')\n",
        "#torch.save({\"token\":tokenizer}, \"drive/MyDrive/Datasets for ORG-4(SentimentAnalysisWithObjectDetection)/tokenizer.pth\")"
      ],
      "metadata": {
        "id": "EXm9XaFmLWzk"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = BertForSequenceClassification.from_pretrained(\"DeepPavlov/rubert-base-cased\", \n",
        "                                                      num_labels = 3, \n",
        "                                                      output_attentions = False,\n",
        "                                                      output_hidden_states = False)\n",
        "model_1.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/model_1.pth'))\n",
        "print(\"Testing:\")\n",
        "model_1.cuda()\n",
        "model_1.eval()\n",
        "\n",
        "t0 = time.time()\n",
        "predictions = torch.Tensor().to(dtype=torch.int8)\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_attention_masks = batch[1].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model_1(b_input_ids, attention_mask=b_attention_masks, output_hidden_states=False, output_attentions=False, return_dict=True)\n",
        "\n",
        "    predictions = torch.cat((predictions, outputs.logits.cpu().argmax(axis=1)))\n",
        "\n",
        "print(classification_report(test_y ,predictions))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-dKV9meN3uH",
        "outputId": "50d711cd-13b5-4f5c-ea52-e86b5365a5a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.70      0.68      5435\n",
            "           1       0.90      0.87      0.89      5278\n",
            "           2       0.76      0.74      0.75      5387\n",
            "\n",
            "    accuracy                           0.77     16100\n",
            "   macro avg       0.78      0.77      0.77     16100\n",
            "weighted avg       0.77      0.77      0.77     16100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def embed_bert_cls(text, model, tokenizer):\n",
        "    t = tokenizer(text,max_length = 65,padding = 'max_length',truncation = True,return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "      model_output = model(**t)\n",
        "    output = model_output.logits\n",
        "    output = torch.nn.functional.softmax(output)\n",
        "    return output, int(output.argmax(axis=1))\n"
      ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
