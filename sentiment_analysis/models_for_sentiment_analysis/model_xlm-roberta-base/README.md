### Описание модели
XLM-RoBERTa — это многоязычная версия RoBERTa. Он предварительно обучен на 2,5 ТБ отфильтрованных данных CommonCrawl, содержащих 100 языков.

RoBERTa — это модель трансформеров, предварительно обученная на большом корпусе в режиме самоконтроля. Это означает, что он был предварительно обучен только на необработанном тексте, и люди не маркировали их каким-либо образом (поэтому он может использовать много общедоступных данных) с автоматическим процессом для создания входных данных и меток из этих текстов.
